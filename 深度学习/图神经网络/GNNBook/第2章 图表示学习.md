图表示学习的目的是将图中的节点分配到低维表示中，并有效地保持图结构。本章系统地介绍传统图表示学习、现代图表示学习和图神经学习

# 1 图表示学习简介

传统的图表示（如邻接矩阵等）给图处理和分析带来了一些挑战。

1. 高计算复杂度

   例如使用两个节点之间的最短或平均路径长度来表示它们的距离。传统的图表示计算距离必须枚举两个节点之间的全部可能路径，本质上是一个组合问题。

   这样的方法导致了很高的计算复杂性

2. 低并行度

   传统图表示的并行度瓶颈在于图中的节点通过边显式地相互耦合，如果将不同节点分布在不同的服务器上会导致服务器之间的通信成本过高。

3. 与机器学习方法的不兼容性

   大多数现成的机器学习方法通常假设数据样本可以用向量空间中独立的向量来表示，而图数据中的样本(即节点)在一定程度上是由边决定的，虽然可以简单地用邻接矩阵的行向量来表示节点，但大型图会导致极高的维数给后续处理带来困难

图表示学习有两个目标。(1) 从学习到的表示空间重构原始图。该目标要求如果两个节点之间有边或者有关系，那么这两个节点在表示空间中的距离应该相对较小。(2) 学习的表示空间可以有效地支持图推理，如进行边预测，识别重要节点，推断节点标签。仅以图重构为目标的表示空间对于图推理是不充分的。

图表示学习方法主要有三大类：传统图嵌入、现代图嵌入和图神经网络

## 2 传统图嵌入

图嵌入通常有两个目标：重建原始图结构和支持图推理。传统的图嵌入方法的目标函数主要针对图重构的目标。

Tenenbaum等人(2000)首先使用K近邻(KNN)等连通性算法构建邻域图G。然后根据G计算不同数据之间的最短路径。因此对于数据集中所有N个数据项，有一个图的距离矩阵。最后对矩阵采用经典的多维尺度法(MDS)得到坐标向量。等距特征映射(ISOMAP)学习到的表示近似地保持了低维空间中节点对的测地线距离。ISOMAP的关键问题在于其逐对计算最短路的高复杂度。局部线性嵌入(LLE) (Roweis and Saul, 2000)被提出，以消除计算远距离节点间距离的需要。LLE假设每个节点及其邻居位于流形的局部线性平面上。为了刻画局部几何的特征，每个节点可以从它的邻居重建。最后在低维空间中LLE基于局部线性重建构造了一个保持邻域映射。

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/LLE.png" alt="LLE" style="zoom: 33%;" />

拉普拉斯特征映射(LE)(Belkin and Niyogi, 2002)也从使用$\epsilon-$邻域或K最近邻来构造图开始。然后是热核(heat kernel)(Berline et al, 2003)选取图中两个节点的权值。最后基于拉普拉斯矩阵正则化得到节点表示。此外，局部保持投影(LPP) (Berline等)提出了非线性LE的一个线性逼近。

可以发现，传统的图嵌入大多适用于由特征表示的数据集构造的图，其中由边权编码的节点之间的距离在原始特征空间中得到很好的定义。与此相反，现代图嵌入大多是在自然形成的网络上，如社交网络等。在这些网络中节点之间的距离是不明确的，没有直接定义。例如，两个节点之间的边通常只表明它们之间有关系，但不能表明具体的接近程度。同样，即使两个节点之间没有边，也不能说这两个节点之间的接近度为零。节点临近度的定义取决于具体的分析任务和应用场景。因此，现代图嵌入方法通常包含丰富的信息，如网络结构、属性、边信息和高级信息，以方便不同的问题和应用。现代的图嵌入需要同时满足前面提到的两个目标。鉴于此，传统的图嵌入可以看作是现代图嵌入的一个特例，而现代图嵌入的最新研究进展更多地关注网络推理。

# 3 现代图嵌入

为了更好地支持网络推理，现代图嵌入考虑了图中更丰富的信息。根据图表示学习中保存的信息类型，现有的方法可以分为三类:(1)图结构和属性保持图嵌入;(2)带有附加信息的图表示学习;(3)高级信息保持图表示学习。从技术角度看，采用不同的模型来合并不同类型的信息或处理不同的目标。常用的模型包括矩阵分解、随机游走、深度神经网络及其变种。

## 3.1 保持结构-属性的图表示学习

在图中编码的所有信息中，图的结构和属性是影响图推理的两个关键因素。因此，图表示学习的一个基本要求是适当保留图的结构和捕获图的属性。通常图结构包括一阶结构和高阶结构，如二阶结构和社团结构。不同类型的图具有不同的属性。例如，有向图具有非对称传递性。结构平衡理论被广泛应用于符号图(signed graphs)

### 3.1.1 保持结构的图表示学习

图结构可以以不同的粒度分为不同的组。图表示中常用的图结构包括邻域结构、高阶节点邻近性和图社区结构

怎样定义邻居结构是第一个挑战。在发现短距离随机游走中节点分布类似于自然语言中单词的分布的基础上，DeepWalk (Perozzi等人，2014)采用随机游走来捕捉邻域结构。然后对于每个随机游走生成的游走序列，在Skip-Gram之后，DeepWalk的目标是最大化游走序列中出现邻居节点的概率。Node2vec定义了一个灵活的节点图邻域概念，并设计了一个二阶随机游走策略对邻域节点进行采样，该策略可以在宽度优先采样(BFS)和深度优先采样(DFS)之间进行平滑插值。除了邻域结构，LINE (Tang et al, 2015b)被提出用于大规模的网络嵌入，它可以保留一阶和二阶邻近性。一阶邻近是观察的两个节点之间的两两接近。二阶邻近度由两个节点的“上下文”(邻居)的相似性决定。两者在测量两个节点之间的关系时都很重要。LINE本质上是基于浅层模型的，因此表示能力有限。SDNE (Wang et al, 2016)提出了一种用于网络嵌入的深度模型，该模型也旨在捕获一阶和二阶邻近性。SDNE采用深度自编码器结构，具有多个非线性层，以保存二阶邻近性。为了保存一阶邻近性，采用了拉普拉斯特征映射的思想。Wang等人(2017)提出了一种模块化的非负矩阵分解方法(M-NMF)模型用于图表示学习，该模型旨在同时保留微观结构，即节点的一阶和二阶邻近性，以及介观社区结构(Girvan和Newman, 2002)。他们采用NMF模型(F´evotte和Idier, 2011)来保存微观结构。同时，采用模块度最大值法的方法检测社区结构(Newman,2006)。然后他们引入一个辅助的社区表示矩阵来连接节点和社区结构的表示。这样学习到的节点表示既受到微观结构的约束，又受到社团结构的约束。

综上所述，许多网络嵌入方法都是为了在潜在的低维空间中保持节点的局部结构，包括邻域结构、高阶邻近性以及社区结构。同时对线性和非线性模型进行了尝试，显示了深度模型在网络嵌入中的巨大潜力。

### 3.1.2保持属性的图表示学习

目前已有的保持属性的图表示学习方法主要关注所有类型图的图传递性和有符号图的结构平衡性。

传递性通常存在于图中。但同时可以发现保持这样的属性并不具有挑战性，因为在度量空间中不同数据点之间的距离自然满足三角形不等式。然而这在现实世界中并不总是正确的。Ou等人(2015)的目标是通过潜在相似性分量来保持非传递性。非传递性属性声明，对于图中节点v1、v2和v3，其中(v1;v2)和(v2;v3)是相似的一对，(v1;V3)可能是不同的一对。例如，在一个社交网络中，一个学生可能与他的同学和家人联系，而他的同学和家人可能非常不同。其主要思想是它们学习多个节点嵌入，然后根据多个相似点(而不是一个相似点)比较不同的节点。他们观察到，如果两个节点的语义相似性很大，那么至少有一个结构相似性很大，否则，所有的相似性都很小。在有向图中，它通常具有非对称传递性。不对称传递性表明，如果从节点i到节点j有一条有向边，从j到v有一条有向边，那么很可能从i到v有一条有向边，而从v到i没有。为了测量这种高阶邻近性，HOPE(Ou等, 2016)在一般公式中总结了四种测量方法，然后利用广义SVD问题来分解高阶接近度(Paige和Saunders, 1981)，使得HOPE的时间复杂度大大降低，这意味着HOPE对于大规模网络具有可扩展性。在具有正边和负边的带符号图中，社会理论，如结构平衡理论(Cartwright和Harary,1956;Cygan等人，2012)，与无符号图有很大的不同。结构平衡理论表明，带符号社交网络中的用户应该能够让他们的“朋友”比他们的“敌人”更亲密。为了对结构平衡现象建模，SiNE (Wang et al, 2017f)使用一个由两个具有非线性函数的深度图组成的深度学习模型。

在网络嵌入空间中，保持网络属性，特别是对网络演化和形成有重要影响的属性的重要性已经得到了广泛的认识。关键的挑战是如何解决原始网络空间和嵌入向量空间在属性层面上的差异和异质性。一般情况下，大多数结构和属性保持方法都考虑了节点的高阶邻近性，这说明了在网络嵌入中保持高阶结构的重要性。区别在于获得高阶结构的策略。一些方法通过假设从一个节点到它的邻居的生成机制隐式地保持高阶结构，而其他一些方法通过显式地逼近嵌入空间中的高阶近邻来实现这一点。由于拓扑结构是网络最显著的特征，保持结构的网络方法在文献中占据了很大一部分。相对而言，保持属性的网络嵌入是一个比较新的研究课题，研究较少。网络属性通常会推动网络的形成和演化，因此它具有很大的研究和应用潜力。