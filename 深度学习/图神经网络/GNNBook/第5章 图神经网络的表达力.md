# 1 介绍

机器学习问题可以被抽象为学习从一些特征空间到目标空间的映射$f^*$。解决这一问题的典型方法是通过优化参数$\theta$来近似$f^*$的模型$f_\theta$。在实践中真实值$f^*$是一个通常未知的先验。因此模型$f_\theta$可能会近似一个相当大范围的$f^*$。对这一范围的估计被称为模型的表达能力，这为模型潜力提供了一个重要的衡量标准。我们希望模型具有更强的表达能力，可以学习更复杂的映射函数。

神经网络以其强大的表达能力而闻名。Cybenko(1989)首先证明了在紧空间(compact space)上定义的任何连续函数都可以用具有sigmoid激活函数和只有一个隐层的神经网络一致逼近。后来，这个结果被(Hornik et al, 1989)推广到任何挤压激活函数(squashing activation functions)。

然而，这些开创性的发现还不足以解释目前网络神经网络在实践中前所未有的成功，因为它们强大的表达能力只表明模型$f_\theta$能够近似$f^*$，但不能保证通过训练得到的模型$\hat{f}$确实近似$f^*$。基于神经网络的方法可能只在数据充足的情况下优于传统方法。一个重要的原因是作为机器学习模型的神经网络仍然受数据量和模型复杂性之间的基本权衡所支配。尽管神经网络具有很强的表达能力，但当与更多参数配对时，它们很可能会过度拟合训练示例。因此，实践中有必要构建在对其参数进行约束的情况下，能够保持较强表达能力的神经网络。同时，需要从理论上对神经网络的表达能力有良好的理解，限制其参数。

在实践中，参数的约束通常是从我们对数据的先验知识中获得的;这些被称为归纳偏差。具有归纳偏差的神经网络的表达能力最近得到了证实。Yarotsky (2017);Liang和Srikant(2017)已经证明深度神经网络通过叠加多个隐藏层，可以比浅层神经网络在显著减少参数的情况下获得足够好的逼近。DNN的架构利用了数据通常具有层次结构的事实。DNN与数据类型无关，而专门的神经网络架构已经开发出来支持特定类型的数据。RNN，CNN分别用于处理时间序列和图像。在这两种类型的数据中，有效模式通常分别在时间和空间上保持平移不变性。为了匹配这种不变性，RNN和CNN采用了其参数在时间和空间上共享的归纳偏差。参数共享机制是对参数的一种约束，限制了RNN和CNN的表达能力。然而，RNN和CNN已经被证明有足够的表达能力来学习平移不变函数(Siegelmann和Sontag, 1995;Cohen和Shashua, 2016;Khrulkov et al, 2018)，这导致了RNN和CNN在处理时间序列和图像方面的巨大实际成功。

图结构数据的机器学习背后的一个基本假设是，预测的目标应该不受图上节点的顺序的影响。为了匹配这一假设，GNN保持一个普遍的归纳偏差称为排列不变性。即GNN的输出应该独立于图的节点索引的分配方式以及处理顺序。

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/permutation%20invariance..png" alt="image-20220722222031461" style="zoom:50%;" />

GNN要求参数独立于节点排序，并且在整个图中共享。由于这种新的参数共享机制，需要新的理论工具来表征它们的表达能力。

分析GNN的表达能力是一个挑战，因为这个问题与图论中一些长期存在的问题密切相关。为了理解这种联系，考虑下面的例子，GNN如何预测一个图结构是否对应于一个有效的分子。GNN应该能够识别这个图结构是否与已知对应有效分子的图结构相同、相似或非常不同。衡量两个图是否具有相同的结构涉及解决图同构问题，其中尚未发现P解(Helfgott et al, 2017)。此外，测量两个图是否具有相似的结构需要处理图编辑距离问题，这比图同构问题更难处理(Lewis et al, 1983)。
