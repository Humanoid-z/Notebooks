本章关注图的一个基本任务:节点分类。下面将给出节点分类的详细定义，并介绍一些经典的方法，如标签传播。随后将介绍几个具有代表性的用于节点分类的图神经网络结构。进一步指出训练深度图神经网络的主要难点——过度平滑(oversmoothing)问题，并介绍一些在这方面的最新进展，如连续图神经网络。

# 1 背景及问题定义

图的一个基本任务是节点分类，它试图将节点分类为几个预定义的类别。例如，在社交网络中，我们想要预测每个用户的政治偏见;在蛋白质相互作用网络中，我们感兴趣的是预测每个蛋白质的功能作用。要进行有效的预测，关键是要有有效的节点表示，这在很大程度上决定了节点分类的性能。

问题定义：G = (V,E)代表一个图，V为点集，E为边集。$A\in R^{N\times N}$代表邻接矩阵，N为节点数，$X\in R^{N\times C}$代表节点特征矩阵，C为特征维度。GNN目标为通过结合图结构信息和节点特征学习有效的节点表示（以$X\in H^{N\times F}$表示，F为节点表示的维度），进一步用于节点分类。

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/20220720170152.png" alt="image-20220720170152653" style="zoom: 67%;" />

# 2 监督图学习

## 2.1 图神经网络一般框架

图神经网络的基本思想是结合相邻节点的表示和自己的表示迭代更新节点表示。

本节将介绍图神经网络的一般框架。从初始节点表示$H^0=X$开始，在每一层中我们有两个重要的函数

AGGREGATE：试图聚合来自每个节点邻居的信息;

COMBINE：试图通过将来自邻居的聚合信息与当前节点表示相结合来更新节点表示。

数学形式上定义GNN一般框架如下：

初始化：$H^0=X$

$For k=1,2,...,k,$
$$
a^k_v=AGGREGATE^k\{H^{k-1}_u:u\in N(v)\}\\
H^k_v=COMBINE^k\{H^{k-1}_u,a^k_v\}
$$
其中N(v)为第v个节点的邻居集合。最后一层的节点表示$H^K$可以作为最终的节点表示

获得节点表示后可以用于下游任务。以节点分类为例，通过Softmax函数可以预测节点v的标签(用$\hat{y}_v$表示)
$$
\hat{y}_v=Softmax(WH^T_v)
$$
其中$W\in \mathbb{R}^{|L|\times F}$为输出空间中标签的个数。

给定一组标记节点，通过最小化以下损失函数来训练整个模型:
$$
O=\frac{1}{n_l}\sum_{i=1}^{n_l}loss(\hat{y}_i,y_i)
$$
其中$y_i$为节点i的真实标签,$n_l$为被标记节点的数量，loss(·,·)为交叉熵等损失函数。通过反向传播最小化目标函数O来优化整个神经网络。

## 2.2图卷积网络

我们将从图卷积网络(GCN) (Kipf和Welling，2017b)开始，由于其在各种任务和应用中的简单和有效，它是现在最流行的图神经网络架构。具体来说，每一层的节点表示按照以下传播规则进行更新:
$$
H^{k+1}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{k} W^{k}\right)
$$
$\tilde{A}=A+\bold I$为无向图G带自连接的邻接矩阵，能在更新节点表示时结合使用节点自身的表示。$\bold I\in \mathbb R^{N\times N}$为单位阵。$\tilde{D}$为对角矩阵，有$\tilde{D}_{ii}=\sum_{j}\tilde{A}_{ij}$(保存节点i的度数)。$\sigma(\cdot)$为激活函数。$W^k\in \mathbb R^{F\times F'}$为每一层可学习的线性变换矩阵（$F,F'$分别为第k，k+1层节点表示维度）

GCN中定义的AGGREGATE和COMBINE函数，对于节点i，节点更新方程可以重新表述如下:
$$
H_{i}^{k}=\sigma\left(\sum_{j \in\{N(i) \cup i\}} \frac{\tilde{A}_{i j}}{\sqrt{\tilde{D}_{i i} \tilde{D}_{j j}}} H_{j}^{k-1} W^{k}\right) \\
H_{i}^{k}=\sigma\left(\sum_{j \in N(i)} \frac{A_{i j}}{\sqrt{\tilde{D}_{i i} \tilde{D}_{j j}}} H_{j}^{k-1} W^{k}+\frac{1}{\tilde{D}_{i}} H_{i}^{k-1} W^{k}\right)
$$
AGGREGATE函数被定义为邻居节点表示的加权平均。邻居j的权值由i和j之间的边的权值决定(即两个节点度归一化的$A_{ij}$)。COMBINE函数定义为聚合消息和节点表示本身的总和，其中节点表示按其自身的度数进行归一化。

## 2.3图注意力网络

在GCNs中，对于目标节点i，邻居j的重要性由其边$A_{ij}$的权值决定(按其节点度归一化)。然而在实践中，输入图可能是有噪声的。边缘权值可能不能反映两个节点之间的真实强度。因此，更有原则的方法是自动学习每个邻居的重要性。图表注意力网络(Graph Attention Networks)GAT(Veliˇckovi´c et al, 2018)是基于这个想法，并试图基于注意力机制学习每个邻居的重要性。

**图注意力层**：图注意力层定义了怎样从第k-1层的隐藏节点表示（用$H^{k-1}\in \mathbb R^{N\times F}$表示）转移到新的节点表示$H^{k}\in \mathbb R^{N\times F'}$。为了保证有足够的表达能力将低级节点表示转换为高级节点表示，对每个节点应用一个共享的线性变换，用$W\in \mathbb R^{F\times F'}$表示。然后在节点上定义自注意力，通过共享注意力机制$a:R^{F'}\times R^{F'}\rightarrow R$来度量任意一对节点的注意系数
$$
e_{ij}=a(WH^{k-1}_i,WH^{k-1}_j)
$$
$e_{ij}$表示节点i与j之间的关系强度。注意本节使用$H^{k-1}_i$表示列向量而不是行向量。对于每个节点，理论上可以允许它关注图上的每个其他节点，但这将忽略图结构信息。更合理的解决方案是只关注每个节点的邻居。在实践中，只使用一阶邻居(包括节点本身)。为了使系数在不同节点之间具有可比性，注意力系数通常用softmax函数进行归一化:
$$
\alpha_{i j}=\operatorname{Softmax}_{j}\left(\left\{e_{i j}\right\}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{l \in N(i)} \exp \left(e_{i l}\right)}
$$
对于节点i，$\alpha_{i j}$本质上定义了相邻节点上的多项分布，也可以解释为从节点i到每个相邻节点的转移概率。

在Veliˇckovi´c等人(2018)的工作中，注意力机制a被定义为一个单层前驱神经网络，包括一个带权向量$W_{2}\in \mathbb R^{1\times 2F'}$的线性变换和一个LeakyReLU非线性激活函数

(负输入斜率$\alpha$=0.2)。可以用以下架构来计算注意力系数:
$$
\alpha_{i j}=\frac{\exp \left(\operatorname{LeakyReLU}\left(W_{2}\left[W H_{i}^{k-1} \| W H_{j}^{k-1}\right]\right)\right)}{\sum_{l \in N(i)} \exp \left(\operatorname{LeakyReLU}\left(W_{2}\left[W H_{i}^{k-1} \| W H_{l}^{k-1}\right]\right)\right)}
$$
式中||表示两个向量concat的运算。新的节点表示是相邻节点表示的线性组合，其权重由注意系数确定(具有潜在的非线性变换)，即。
$$
H_{i}^{k}=\sigma\left(\sum_{j \in N(i)} \alpha_{i j} W H_{j}^{k-1}\right)
$$
**多头注意力**：

在实践中可以使用多头注意力机制，每个多头注意力在节点上决定不同的相似函数。对于每个注意力头，可以根据下式独立地获得一个新的节点表示。最终的节点表示将是不同注意力头学习到的节点表示的concat。
$$
H_{i}^{k}=\|_{t=1}^{T} \sigma\left(\sum_{j \in N(i)} \alpha_{i j}^{t} W^{t} H_{j}^{k-1}\right)
$$
式中T为注意力头总数，$\alpha_{i j}^{t}$为从第T个注意力头计算出的注意系数，$W^{t}$为第T个注意力头的线性变换矩阵。

Veliˇckovi´c等人(2018)在论文中提到，在最后一层试图组合来自不同注意头的节点表示时，可以使用其他池化技术，而不是使用连接操作，例如简单地从不同注意头中提取平均节点表示。
$$
H_{i}^{k}=\sigma\left(\frac{1}{T} \sum_{t=1}^{T} \sum_{j \in N(i)} \alpha_{i j}^{t} W^{t} H_{j}^{k-1}\right)
$$

## 2.4神经消息传递网络

另一个非常流行的图神经网络架构是神经消息传递网络(MPNN) (Gilmer et al, 2017)，最初被提出用于学习分子图表示。然而MPNN实际上是非常通用的，它提供了一个图神经网络的通用框架，也可以用于节点分类的任务。MPNN的基本思想是将现有的图神经网络作为神经信息在节点之间传递的一般框架。在MPNN中，有两个重要的功能:Message和Updating:
$$
m_{i}^{k}=\sum_{i \in N(j)} M_{k}\left(H_{i}^{k-1}, H_{j}^{k-1}, e_{i j}\right), \\
H_{i}^{k}=U_{k}\left(H_{i}^{k-1}, m_{i}^{k}\right) .
$$
$ M_{k}(\cdot, \cdot, \cdot)$定义了第k层节点i和j之间的消息，它依赖于两个节点的表示以及它们边的信息。$U_{k}$是第k层的节点更新函数，它结合了来自邻居的聚合消息和节点表示本身。

## 2.5连续图神经网络

上述图神经网络用不同类型的图卷积层迭代更新节点表示。本质上这些方法用GNN为节点表示的离散动态建模。Xhonneux等人(2020)提出了连续图神经网络(CGNNs)，该网络将现有具有离散动态的图神经网络推广到连续动态，即试图对节点表示的连续动态进行建模。其核心思想是如何刻画节点表示的连续动态，即节点表示随时间变化的导数。CGNN模型的灵感来自于基于扩散的图模型，如PageRank和社交网络上的流行病模型。节点表示的导数被定义为节点表示本身、它的邻居表示以及节点的初始状态的组合。具体地说，CGNN引入了两种不同的节点动态变体。第一个模型假设节点表示的不同维度(又称特征通道)是独立的;第二个模型更灵活，它允许不同的特征通道相互交互。接下来分别对这两种模型进行介绍。

**注意**：本节不使用原来的邻接矩阵A，而是使用以下正则化矩阵来表示图结构:
$$
A:=\frac{\alpha}{2}\left(I+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right)
$$
其中$\alpha \in (0,1)$是超参数。D是原始邻接矩阵A的度矩阵，通过新的正则化邻接矩阵A, A的特征值位于区间$[0,\alpha]$，当k增加时，$A^k$会收敛到0。

**模型1:独立的特征通道**。由于图中不同的节点是相互连接的，所以要对每个特征通道的动态建模，自然需要考虑图结构，它允许信息在不同的节点之间传播。我们的动机是基于图的现有扩散方法，如PageRank (Page等人，1999)和标签传播(Zhou等人，2004)，它们用以下逐步传播方程定义了节点表示(或节点上的信号)的离散传播
$$
H^{k+1}=AH^k+H^0
$$
其中$H^0=X$或者是编码器在输入特征X上的输出。在每一步新的节点表示是相邻节点表示和初始节点特征的线性组合。这种机制允许在不忘记初始节点特征的情况下对图上的信息传播建模。第k步的节点表示如下:
$$
H^{k}=\left(\sum_{i=0}^{k} A^{i}\right) H^{0}=(A-\mathbf{I})^{-1}\left(A^{k+1}-\mathbf{I}\right) H^{0}
$$
由于上述方程有效地建模了节点表示的离散状态，CGNN模型将其进一步扩展到连续状态，将离散时间步长k替换为连续变量$t\in \mathbb R^{+}_0$。具体来说上式是以下常微分方程(ODE)的离散化:
$$
\frac{d H^{t}}{d t}=\log A H^{t}+X
$$
初始值$H^0=(\log AH^t)^{-1}(A-I)X$，其中X是初始节点特征或应用于它的编码器的输出。由于logA在实际中难以计算，所以近似于一阶泰勒展开，即$\log A \approx A-I $。通过整合所有这些信息，我们得到下面的ODE方程:
$$
\frac{d H^{t}}{d t}=(A-\mathbf{I}) H^{t}+X
$$
初始值$X^0=X$ ，这是CGNN模型的第一种变体

CGNN模型其实是非常直观的，它与传统的传染病模型有很好的联系，传统的传染病模型旨在研究人群感染的动态。对于流行病模型，通常假设人的感染会受到来自邻居的感染、自然恢复和人的自然特征三种不同因素的影响。如果我们将$H^t$作为时刻t的感染人数，那么这三个因素可以自然地用上式中的三项来建模：$AH^t$为邻居感染，$-H^t$为自然恢复，最后X为人的自然特征。

**模型2:特征通道交互建模**。上述模型假设不同的节点特征通道相互独立，这是一个非常强的假设，限制了模型的容量。受图神经网络线性变体(即Simple GCN (Wu等人，2019a))成功的启发，提出了一种更强大的离散节点动态模型，该模型允许不同的特征通道相互作用，
$$
H^{k+1}=A H^{k} W+H^{0}
$$
其中$W\in \mathbb R^{F\times F}$为权重矩阵，用于对不同特征通道之间的交互进行建模。同样，我们也可以将上述离散状态扩展到连续情况，得到如下方程:
$$
\frac{d H^{t}}{d t}=(A-\mathbf{I}) H^{t}+H^{t}(W-\mathbf{I})+X
$$
初始值为$X^0=X$，这是CGNN的第二种可训练权值变体。两个矩阵$A-\mathbf{I}$和$W-\mathbf{I}$描述了系统的自然解，而X是提供给系统的信息，使系统进入所需状态。

**讨论**。提出的连续图神经网络(CGNN)有多个优良的性质:

(1)最近的工作表明，如果我们增加层数，在离散图神经网络K中，学习到的节点表示容易出现过平滑的问题，因此失去了表达能力。而连续图神经网络能够训练非常深的图神经网络，并在实验上对任意选择的积分时间具有鲁棒性;

(2)对于图上的一些任务，对节点之间的远程依赖关系进行建模是至关重要的，这需要训练深度GNN。现有的离散GNN由于过度平滑问题，无法训练出非常深的GNN。CGNN由于具有时间稳定性，能够有效地模拟节点之间的长程依赖关系。

(3)超参数$\alpha$非常重要，它控制着扩散速率。具体地说，它控制正则化矩阵A的高阶幂消失的速率。在(Xhonneux等人，2020)提出的工作中，作者建议学习每个节点的不同值的$\alpha$，从而允许为不同的节点选择最佳扩散速率。

# 3 无监督图神经网络

## 3.1 变分图自编码器

继变分自编码器(VAEs) (Kingma和Welling, 2014;Rezende等人，2014)，变分图自编码器(VGAEs) (Kipf和Welling, 2016)提供了一个对图结构数据进行无监督学习的框架。

### 3.1.1问题设置

假设给定一个无向图G = (V,E)，有N个节点。每个节点都有一个节点特征向量。将所有节点特征紧凑表示为一个矩阵$X\in \mathbb R^{N\times C}$。图的邻接矩阵是A，假设在原图G上加入了自环，使A的对角线项为1。这是图卷积网络(GCNs) 的惯例，使模型在更新节点的新表示时考虑节点的旧表示。还假设每个节点都有一个潜变量(所有潜变量的集合再次紧凑地表示为一个矩阵$Z\in \mathbb R^{N\times F}$)。我们感兴趣的是推断图中节点的潜在变量和解码边。

### 3.1.2模型

与VAEs类似，VGAE模型由编码器$q_{\phi}(Z \mid A, X)$、解码器$p_{\theta}(A\mid Z)$和先验p(Z)组成。

**编码器**：编码器的目标是在节点特征X和邻接矩阵上学习与每个节点条件相关的潜变量的分布。我们可以实例化$q_{\phi}(Z \mid A, X)$为图神经网络，其中可学习参数为$\phi$。特别地，VGAE假设有一个节点独立的编码器如下，
$$
q_{\phi}(Z \mid X, A) = \prod_{i = 1}^{N} q_{\phi}\left(\mathbf{z}_{i} \mid X, A\right) \\
q_{\phi}\left(\mathbf{z}_{i} \mid X, A\right) = \mathscr{N}\left(\mathbf{z}_{i} \mid \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right) \\
\boldsymbol{\mu}, \boldsymbol{\sigma} = \operatorname{GCN}_{\phi}(X, A)
$$
其中$z_i,\mu_i,\sigma_i$分别为矩阵$Z,\mu,\sigma$的第i行，总体上假设一个多元正态分布，对角协方差作为每个节点(即$z_i$)的潜向量的变分近似分布。平均协方差和对角协方差由编码器网络预测，即GCN。原论文使用的双层GCN如下:
$$
\boldsymbol{\mu}=\tilde{A} H W_{\boldsymbol{\mu}} \\
\boldsymbol{\sigma}=\tilde{A} H W_{\boldsymbol{\sigma}} \\
H=\operatorname{ReLU}\left(\tilde{A} X W_{0}\right)
$$
其中$\tilde{A}=D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$为对称归一化邻接矩阵，D为度矩阵。可学习参数为$\phi=[W_\mu,W_\sigma,W_0]$

**解码器**：给定抽样潜变量，解码器旨在预测节点之间的连通性。原文采用简单的基于点积的预测器如下:
$$
p(A \mid Z) = \prod_{i = 1}^{N} \prod_{j = 1}^{N} p\left(A_{i j} \mid \mathbf{z}_{i}, \mathbf{z}_{j}\right) \\
p\left(A_{i j} \mid \mathbf{z}_{i}, \mathbf{z}_{j}\right) = \boldsymbol{\sigma}\left(\mathbf{z}_{i}^{\top} \mathbf{z}_{j}\right)
$$
其中$A_{i j}$为第(i,j)个元素，$\sigma(\cdot)$为sigmoid函数，为了便于处理，解码器再次假设所有可能的边之间存在条件独立性。注意，这个解码器没有可学习的参数。提高解码器性能的唯一方法是学习良好的潜在表示。

**先验**：潜变量的先验分布被简单地设置为具有单位方差的独立零均值高斯分布，
$$
p(Z)=\prod_{i=1}^{N} \mathscr{N}\left(\mathbf{z}_{i} \mid \mathbf{0}, \mathbf{I}\right)
$$
**目标与学习**：为了学习编码器和解码器，需要最大化证据下界(ELBO)，如VAEs中
$$
\mathscr{L}_{\text {ELBO }}=\mathbb{E}_{q_{\phi}(Z \mid X, A)}[\log p(A \mid Z)]-\operatorname{KL}\left(q_{\phi}(Z \mid X, A) \| p(Z)\right)
$$
式中$\operatorname{KL}(q\| p)$为分布q和p之间的KL散度。注意不能直接最大化对数似然，因为潜变量Z引入了高维积分，这是棘手的。相反，我们最大化上式中的ELBO，这是对数似然的下界。然而，第一个期望项是难以处理的。通常采用蒙特卡洛估计，从编码器$q_{\phi}(Z \mid X, A)$抽样几个Z，并使用样本评估该项。为了使目标最大化，可以使用重参数化技巧(Kingma和Welling, 2014)进行随机梯度下降。注意，重参数化技巧是必要的，因为我们需要通过上述蒙特卡洛估计项中的采样反向传播来计算编码器参数的相关梯度。

### 3.1.3 讨论

VGAE模型由于其简单和良好的经验性能而在文献中得到广泛的应用。例如，由于先验和解码器没有可学习的参数，模型是相当轻量级的，学习过程比较快。此外，VGAE模型是通用的，一旦我们学习了一个好的编码器，即好的潜在表示，我们可以使用它们来预测边(链接预测)、节点属性，等等。另一方面，VGAE模型在以下方面仍然有局限性。首先，由于解码器是不可学习的，所以它不能像VAEs那样作为图形的生成模型。人们可以简单地设计一些可学习的解码器。然而，我们并不清楚学习良好的潜在表示和生成具有良好品质的图的目标是否总是一致的。沿着这个方向进行更多的探索将会富有成效。其次，编码器和解码器都利用独立性假设，这可能是非常有限的。为了提高模型的能力，需要更多的结构依赖(例如，自回归)。第三，正如在原始论文中所讨论的，先验可能是一个糟糕的选择。最后，对于实际的链路预测，可能需要在解码器中增加边与非边的权重，并仔细调整它，因为图可能非常稀疏。

## 3.2Deep Graph Infomax

在Mutual Information Neural Estimation(MINE) (Belghazi等人，2018)和Deep Infomax (Hjelm等人，2018)之后，Deep Graph Infomax (Veliˇckovi´c等人，2019)是一种无监督学习框架，通过互信息最大化的原则学习图表示。

### 3.2.1问题设置

跟随原文，我们将在单图背景下解释模型，即提供单图的节点特征矩阵X和图邻接矩阵A作为输入。对其他问题设置的扩展，如转导和归纳学习设置将在第3.2.3节讨论。目标是以无监督的方式学习节点表示。在学习了节点表示之后，可以在表示的基础上应用一些简单的线性(逻辑回归)分类器来执行监督任务，比如节点分类。

### 3.2.2模型

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/Deep%20Graph%20Infomax.png" alt="image-20220722154857251" style="zoom:67%;" />

该模型的主要思想是最大化节点表示(捕获局部图信息)和图表示之间的局部互信息(捕获全局图形信息)。通过这样做，学习到的节点表示应该尽可能地捕捉全局图信息。

首先把图编码器表示为$\varepsilon $，它可以是前面讨论过的任何GNN，如一个两层GCN。得到所有节点表示为$H=\varepsilon(X,A)$，其中任何节点i的表示$h_i$都应该包含一些靠近节点i的局部信息。具体来说，k层GCN应该能够利用k-hop距离的节点信息。为了获得全局图信息，可以使用readout层/函数来处理所有节点表示，例如，$\mathbf{s}=\mathscr{R}(H)$，其中读出函数R可以是一些可学习的池化函数或简单的平均操作符。

**目标**：给定本地节点表示$h_i$和全局图表示s，下一步是计算它们的互信息。互信息的定义如下：
$$
\operatorname{MI}(\mathbf{h}, \mathbf{s})=\iint p(\mathbf{h}, \mathbf{s}) \log \left(\frac{p(\mathbf{h}, \mathbf{s})}{p(\mathbf{h}) p(\mathbf{s})}\right) \mathrm{d} \mathbf{h} \mathrm{d} \mathbf{s}
$$
然而，仅最大化局部互信息还不足以学习有用的表示。为了制定一个更实用的目标，Veliˇckovi´c等人在Deep Infomax (Hjelm等人，2018)之后使用了噪声对比型目标，
$$
\mathscr{L}=\frac{1}{N+M}\left(\sum_{i=1}^{N} \mathbb{E}_{(X, A)}\left[\log \mathscr{D}\left(\mathbf{h}_{i}, \mathbf{s}\right)\right]+\sum_{j=1}^{M} \mathbb{E}_{(\tilde{X}, \tilde{A})}\left[\log \left(1-\mathscr{D}\left(\tilde{\mathbf{h}}_{j}, \mathbf{s}\right)\right)\right]\right)
$$
其中$\mathscr{D}$是一个二元分类器，它以节点表示$h_i$和图表示s作为输入，并预测$h_i$是来自联合分布$p(\mathbf{h}, \mathbf{s})$(正类)还是来自边缘分布$p(\mathbf{h_i}) p(\mathbf{s})$(负类)的乘积。以$\tilde{\mathbf{h}}_{j}$作为来自负样本的第j个节点表示。正样本数为N，负样本数为M。因此，总体目标是训练概率分类器的负二元交叉熵。注意，这个目标与生成对抗网络中使用的距离类型相同(GANs) (Goodfellow等人，2014b)，与Jensen-Shannon散度成比例。由(Hjelm等人，2018)证明，最大化基于Jensen-Shannon散度的互信息估计量的行为与直接最大化互信息相似(即，它们具有近似单调的关系)。因此，使式(4.42)中的目标最大化，就有望使互信息最大化。此外，自由选择负样本使该方法更有可能学习有用的表示，而不是最大化原本的互信息。

**负采样**：为了生成正样本，可以直接从图中抽取几个节点来构建($h_i$, s)，对于负样本，可以通过破坏原始图数据来生成，表示为$(\tilde{X},\tilde{A})=\mathscr{C}(\tilde{X},\tilde{A})$。在实践中，人们可以选择破坏函数C的各种形式。例如，作者(Veli ckovi c等人，2019)建议保持邻接矩阵相同，并通过打乱行序破坏节点特征X。破坏函数的其他可能包括随机抽样子图和对节点特征应用Dropout (Srivastava等人，2014)。

收集了正样本和负样本后，可以通过最大化上式中的目标来学习表示。Deep Graph Infomax的训练过程如下:

通过破坏函数采样负样本$(\tilde{X},\tilde{A})\sim \mathscr{C}(\tilde{X},\tilde{A})$。

计算正样本的节点表示$H =\{h_1,···,h_N\}=\varepsilon(X,A)$。

计算负样本的节点表示$\tilde{H} =\{\tilde{h}_1,···,\tilde{h}_N\}=\varepsilon(\tilde{X},\tilde{A})$。

通过读出函数$\mathbf{s}=\mathscr{R}(H)$计算图表示。

通过梯度上升更新$\varepsilon,\mathscr{D},\mathscr{R}$的参数，使目标函数最大化。

### 3.2.3讨论

Deep Graph Infomax是一种针对图结构数据的高效无监督表示学习方法。编码器、读出和二进制交叉熵损失的实现都是直接的。小批训练不需要存储整个图，因为读出的数据也可以应用于一组子图。因此，该方法是节省内存的。此外，正样本和负样本的处理可以并行进行。此外，作者还证明了在某些条件下，如读出函数是单射的，输入特征来自于一个有限集，可以利用最小化交叉熵类型的分类误差来最大化互信息。然而，破坏函数的选择似乎是至关重要的，以确保满足经验性能。似乎没有一个普遍有效的破坏函数。需要根据任务/数据集进行反复试验以获得合适的破坏函数。

# 4 过平滑问题

通过叠加多层图神经网络来训练深度图神经网络通常会产生较差的结果，这是许多不同的图神经网络结构中常见的问题。这主要是由于过平滑的问题，(Li 等, 2018b)表明图卷积网络是拉普拉斯平滑的一种特殊情况:
$$
Y=(1-\gamma I) X+\gamma \tilde{A}_{r w} X
$$
其中$\tilde{A}_{r w}=\tilde{D}^{-1} \tilde{A}$，它定义图上节点之间的变换概率。GCN对应于$\gamma=1$和对称矩阵$\tilde{A}_{s y m}=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$的拉普拉斯平滑的特殊情况。拉普拉斯平滑法使属于同一簇的节点得到相似的表示，有利于节点分类等下游任务。然而当GCNs越深，节点表示就会出现过平滑的问题，即所有节点都有相似的表示。因此下游任务也会收到影响。

**PairNorm**：PairNorm用于缓解GNN层数变深的过平滑问题。PairNorm的基本思想是保持节点表示的总成对平方距离(TPSD)不变，这与原始节点特征X的相同。图卷积节点表示的输出$\tilde{H}$作为PairNorm的输入，PairNorm输出$\hat{H}$。PairNorm的目标是归一化$\tilde{H}$，这样归一化后$TPSD(\hat{H})=TPSD(X)$。
$$
\sum_{(i, j) \in \mathscr{E}}\left\|\hat{H}_{i}-\hat{H}_{j}\right\|^{2}+\sum_{(i, j) \notin \mathscr{E}}\left\|\hat{H}_{i}-\hat{H}_{j}\right\|^{2}=\sum_{(i, j) \in \mathscr{E}}\left\|X_{i}-X_{j}\right\|^{2}+\sum_{(i, j) \notin \mathscr{E}}\left\|X_{i}-X_{j}\right\|^{2} .
$$
在实践中，(Zhao and Akoglu, 2019)提出在不同的图卷积层中保持TPSD值C不变，而不是测量原始节点特征X的TPSD值。C是PairNorm层的超参数，可以针对每个数据集进行调优。为了将$\tilde{H}$归一化到有固定TPSD值的$\hat{H}$，必须首先计算TPSD($\tilde{H}$)。然而这种计算非常昂贵，复杂度为$O(n^2)$。TPSD可以重新表述为:
$$
\operatorname{TPSD}(\tilde{H})=\sum_{(i, j) \in[N]}\left\|\tilde{H}_{i}-\tilde{H}_{j}\right\|^{2}=2 N^{2}\left(\frac{1}{N} \sum_{i=1}^{N}\left\|\tilde{H}_{i}\right\|_{2}^{2}-\left\|\frac{1}{N} \sum_{i=1}^{N} \tilde{H}_{i}\right\|_{2}^{2}\right)
$$
可以通过将每个$\tilde{H}_i$减去每行的平均值来简化上述方程。$\tilde{H}_{i}^{c}=\tilde{H}_{i}-\frac{1}{N} \sum_{i=1}^{N} \tilde{H}_{i}$，能够将表示居中。使节点表示居中的一个很好的特性是，它不会改变TPSD，同时把第二项置零。结果为：
$$
\operatorname{TPSD}(\tilde{H})=\operatorname{TPSD}\left(\tilde{H}^{c}\right)=2 N\left\|\tilde{H}^{c}\right\|_{F}^{2}
$$
综上所述，PairNorm可以分为两个步骤:居中和缩放，
$$
\tilde{H}_{i}^{c}=\tilde{H}_{i}-\frac{1}{N} \sum_{i=1}^{N} \tilde{H}_{i}$ \\
\hat{H}_{i}=s \cdot \frac{\tilde{H}_{i}^{c}}{\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left\|\tilde{H}_{i}^{c}\right\|_{2}^{2}}}=s \sqrt{N} \cdot \frac{\tilde{H}_{i}^{c}}{\sqrt{\left\|\tilde{H}^{c}\right\|_{F}^{2}}}
$$
其中s是决定C的超参数，最后有
$$
\operatorname{TPSD}(\hat{H})=2 N\|\hat{H}\|_{F}^{2}=2 N \sum_{i}\left\|s \cdot \frac{\tilde{H}_{i}^{c}}{\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left\|\tilde{H}_{i}^{c}\right\|_{2}^{2}}}\right\|_{2}^{2}=2 N^{2} s^{2}
$$
变成跨不同卷积层的常数