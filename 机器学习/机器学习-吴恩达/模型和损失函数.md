# 损失函数

我们可以使用损失函数来衡量假设函数的准确性。

常用的损失函数有平方误差函数

![image-20210816110622107](https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20210816110622107.png)

$$\frac 12$$是为了便于梯度下降的计算，平方在求导后和$$\frac 12$$抵消

让损失函数最小值的参数就是最优拟合曲线

## 梯度下降算法

![image-20210816140844789](https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20210816140844789.png)

j=0,1 表示特征索引号。在每次迭代 j 时，应该同时更新参数

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20210816141008200.png" alt="image-20210816141008200" style="zoom:33%;" />

取我们的损失函数的导数（函数的切线）。切线的斜率是该点的导数，它将为我们提供前进的方向。我们在下降最陡的方向上降低损失函数。每一步的大小由参数α决定，称为学习率。较小的 α 会导致较小的步长，而较大的 α 会导致较大的步长。

### 介绍

首先梯度下降算法的目的是获得最低的损失函数所处的参数，如果是线性回归算法就是拟合曲线y=θ~1~x+θ~0~ 的参数

主要思想是通过求损失函数的偏导，控制参数在迭代中不断逼近最优点。如果是多维的参数，那么会控制参数向最陡的方向下降落到局部最优点。参数的α是控学习速率的，太小会导致迭代次数过多，太大可能导致无法收敛。