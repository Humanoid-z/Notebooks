# 项目

简要介绍

- 首先主题是基于时空大数据和遗传算法对公交线路进行优化，我设计的技术路线主要是，首先要把数据源也就是手机信令数据用DBSCAN算法聚类，一次处理一个人一天的数据，把形成的簇的中心点设为停留点，我们认为停留点代表着这个人有前往或者离开此处的需求，把停留点按时间排列就得到了一个人的出行需求，整个数据处理完后得到实验区域居民整体的交通需求。其次需要从网上爬取实验区域公交站的位置信息，然后在arcgis软件上把实验区域地图按照泰森多边形的方式以公交站为控制点，进行分区，然后做一个数据融合，就是写脚本用arcgis的api，让停留点获得它所在的分区号，这样就能生成一个OD矩阵，OD矩阵存储的数据代表这些分区之间的一个交通需求，这也是后面优化线路的参考数据。下一步我需要写脚本调百度地图api，获得公交站点之间的行车距离，根据这个数据使用k最短路径算法，得到很多备选的线路。最后综合考虑交通需求满足率，线路长度等目标，通过线性加权设计一个适应度函数，将之前备选路线通过二进制编码，使用遗传算法筛选出一批适应度最优的线路。

**我的部分**

- 我的话，作为队长，首先是看了很多论文，安排整体的技术路线。还有就是写论文，分配任务。

  除了这些以外，我还写了一些脚本，做了数据融合和获取方面的工作。比如说用arcgis的接口把停留点数据映射到不同的分区内，公交之间的行车距离也是我写脚本调百度地图api获得的。

  项目后半部分生成优化线路也是我负责的，我在matlab上实现了k最短路算法和遗传算法，得到了最后的公交线路

**为什么**用**手机信令数据**做数据源

- 因为手机信令数据数据量大、分布广，时间和位置信息记录得比较精确

**什么是**手机信令数据？

- 是手机用户在发生通话、发短信或移动位置时，信号被基站捕获并记录下来的数据

**数据源**

- 我们学校和镇江本地的移动公司有合作关系，老师帮我们要到了京口区7天的手机信令数据，是经过脱敏处理的，就是只有编号，没有身份信息的。
- 条数：200,000人\*7天*100条=1亿多条，经过DBSCAN后只剩200多万
- 大小：id:4b  经纬度8b，时间4b=20b 2,800,000,000b=2,734,375kb=2,670M=**2.6G**

解释一下**手机信令**的**结构**

- 包括编号，经纬度和时间戳

为什么用**DBSCAN**聚类

- 因为信令数据只说明人经过了某个地方，不一定就有要到这里的需求，他可能只是路过。而人如果长时间停留在某处，那他的轨迹点可以被聚类成簇，把簇的中心点设为停留点，就认为停留点代表这个人有前往和离开该停留点附近区域的需求。另外我们根据需要还对DBSCAN算法做了一点改进，DBSCAN有2个重要参数，ε是簇的扫描范围，minPts是簇内点的最小个数，我们又加上一个Δt，规定超过Δt的2个点不能聚在同一个簇内，因为不符合我们停留点的定义，有两个时间跨度很大的点不能认为这个人一直在此处停留。通过DBSCAN 算法可以使一个人一天大概一百多条的数据量压缩到十多条，当然这取决于参数的设置。

什么是**DBSCAN**

- DBSCAN 算法是一种基于密度的聚类算法，它有2个核心参数ϵ扫描半径和minPts簇内最少样本数。根据这2个参数将数据点分为三类：
  1.核心点：在半径Eps内含有超过MinPts数目的点。
  2.边界点：在半径Eps内点的数量小于MinPts,但是落在核心点的邻域内的点。
  3.噪音点：既不是核心点也不是边界点的点。

  将距离不超过邻域半径的核心点及其所属边界点聚成同一个簇。

为什么用**泰森多边形**分区

- 泰森多边形的话我用公交站作为控制点，分区内部任意一点到构成这个多边形的控制点也就是公交站的距离小于到其它多边形的控制点的距离，因为人们都希望去步行距离最短的公交站坐车嘛。之所以有这一步，是为了通过公交站和区域的映射关系，知道这条线路满足了多少需求，从而对线路进行评价。

**k最短路算法**

- 之所以用k最短路而不是最短路是为了增加备选的线路数量，不仅要一个最短路径，还需要次短路径，因为我们是多目标优化，备选线路不能只招最短的。另外用这种算法能保证线路比较直，用术语说就是线路的非直线系数比较低，这里我们是参考了国家设计标准。

**遗传算法**

- 遗传算法是一个全局搜索算法，它是在不断迭代中通过选择操作淘汰差的个体，保留好的个体，同时用交叉变异的操作增加搜索的全局性。而最重要的是他的适应度函数，是个体评价的唯一标准。我在这里设定一个个体有三十条线路，每个线路用二进制编码，首尾相连就是一个个体。一个族群有200个个体，都是随机生成的。而适应度函数这里，它的输入参数是某个个体的编码，我通过解码找到那30条线路，根据线路的长度、交通需求满足率等方面计算出它的适应度。最后正该算法经历了200多代后开始收敛，总共运行了500代。我把收敛后最优的那个个体作为最终的线路。

使用过什么**大数据处理框架**？

- 当时由于时间比较紧，并且案例用的数据也还可以处理，所以就没去搭框架，但是也是去了解了一些相关的框架。比如说**Hadoop**框架，它主要有分布式存储和分布式计算。通过分布式计算框架将其它服务器的资源一起调度。然后数据挖掘可能会用到不同来源的数据，不太可能整合到一起，这也需要用到分布式文件系统。

解释**K最短路径**

- 通过dijkstra算法得到最短线路集合，通过删除路径算法得到次短线路集合

学到了什么

- 我觉得最重要的收获是，我了解到了科研的基本步骤，首先就是要大量阅读相关方面的论文，了解当前领域目前都在做什么，做到哪种程度,用了什么方法，然后根据课题研究的需要从论文中找出相应的理论，进行组合，并且尽可能地提出改进点进行创新，其次就是用这些方法设计一个方案，对所研究的问题提出一个解决的模型，最后是进行实验，收集实验数据，用图表进行可视化的工作。此外的话我还了解到了像聚类算法DBSCAN，遗传算法等，最后就是锻炼了我的论文写作能力、组织合作能力和动手能力。

# 论文

活动图描述活动的顺序，展现从一个活动状态到另一个活动状态的控制流。用于描述一个用例的业务流程

优势：首先传统基于UML模型生成样例的方法，只能从模型中生成抽象的样例，还要手动实现；此外测试用例还是通过遍历模型的path来生成的。模型中的循环和分支使得path非常复杂，导致了算法的高度复杂性。而本文的方法使用间接手段，从大量随机的测试用例中，用tracing和活动图匹配，计算分别的覆盖率，根据标准筛选得到一个精简的测试用例集，不用手动去实现抽象样例

# java

C++和Java的区别？

1. Java源码会先经过编译变成中间码.class文件，中间码再被解释器翻译成机器码。(JIT即时编译器会让热点代码一次翻译成机器语言并储存起来，有点像内存管理的快表

   C++源码一次编译，直接在编译的过程中链接了，形成了机器码。

2. 同样的算法实现，c++效率比java高（因为C++内存是裸露的，直接操作地址，因此好的C++程序员能更精准地进行优化，而java程序员无法精确控制数据结构的布局和生命周期，如果触发自动垃圾回收机制会花很长时间。Java相当于拿了部分效率换安全，比如地址越界，java就会用exception拦截，而C++中可能就直接程序崩溃了。

3. Java是纯面向对象的语言。而C++中还有面向过程的东西，比如全局变量和全局函数。

4. Java可以利用JVM跨平台。Java虚拟机就是负责将字节码文件翻译成特定平台下的机器码然后运行不同平台上安装对应的JVM，就可以运行字节码文件。这是一种降低耦合的思想

Java GC

- Java将对象保存在堆内存中，堆内存分为3个部分：新生代、老年代和永久代,新实例化的对象都在新生代，经历过多次GC存活下来的对象将进入老年代。新生代区域满了触发minor GC，老年代满了触发full GC，同时也触发minor GC，永久代保存常量，这个区域发生Major GC ，只不过条件比较苛刻。对象有一个引用计数器，计数器为0时会在下一次GC中被清理。GC发生时除GC所需的线程外，所有的线程都进入等待状态，这也是导致java效率较低的因素


多态

重写和重载都是java编程中多态性的体现

重写

- 重写是子类重写父类的方法。父类引用指向子类对象，如果通过这个引用调用该方法，被执行的是子类重写后的方法

重载

- 重载是同一个类里，函数名称相同，参数不同的多个方法。


# 计网

## 概念

子网掩码：传统的ip5类地址划分比较死板，容易浪费，所以采用子网掩码来灵活地划分子网，子网掩码与ip地址做与运算区分主机号和网络号。

静态路由：手工配置路由表

动态路由：路由间交换信息，通过算法优化路由项

距离矢量路由：所有结点都定期地将它们的整个路由表传送给**直接相邻**的结点。

链路状态路由：每个参与该算法的结点都具有**完全的网络拓扑信息**，向本自治系统中**所有**路由器发送信息(**泛洪**)。发送的信息是相邻的所有路由器的链路状态。**只有当链路状态发生变化**时，路由器才向所有路由器发送此信息。

mac是电脑中哪个部分的地址，能不能修改？

- mac地址是物理地址，它与网卡绑定，进行数据链路层的通信，一般不能修改

## 协议

NAT(Network Address Translation，网络地址转换)

- NAT协议是**网络层**协议，他的功能是将**私有地址转换为公网ip地址**。要连接到公网的路由器上安装NAT软件，可以将一个公网IP地址对应多个私有地址，按照**端口号**区分。


RIP路由信息协议

- RIP(Routing Information Protocol,路由信息协议)是网络层动态路由协议（动态路由协议就是路由器之间互相通信，传递路由信息，利用收到的路由信息更新路由表），属于**距离矢量路由协议**，使用**跳数**作为**唯一度量**。路由器**仅和相邻**路由器交换信息。而交换的信息是当前路由器**自己的路由表**。


OSPF开放最短路径优先协议

1. OSPF向本自治系统中的所有路由器发送信息，这里使用的方法是洪泛法。而RIP仅向自己相邻的几个路由器发送信息。
2. 发送的信息是与本路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分信息。“链路状态”说明本路由器和哪些路由器相邻及该链路的“度量”(或代价)。而在RIP中，发送的信息是本路由器所知道的全部信息，即整个路由表。
3. 只有当链路状态发生变化时，路由器才用洪泛法向所有路由器发送此信息，并且更新过程收敛得快，不会出现RIP“坏消息传得馒”的问题。而在RIP中，不管网络拓扑是否发生变化，路由器之间都会定期交换路由表的信息。



ARP协议(Address Resolution Protocol,地址解析协议)

- ARP是网络层协议，它的功能是进行**IP地址到物理地址(MAC地址)的映射**


- ARP使用广播还是多播？

  **寻找**目的主机的MAC地址时使用**广播**，返回目的MAC地址时使用**单播**。

DHCP动态主机配置协议

- 给主机分配动态ip地址。C/S方式，需要IP地址的主机启动时向DHCP 服务器**广播**发送发现报文。只有DHCP服务器回答此广播报文。


ICMP网际控制报文协议

- 主机或路由器报告差错或异常的协议,如ping

为什么tcp**三次握手**建立连接？

- 假设客户端发送了第一个请求连接并且没有丢失，只是网络拥塞，滞留时间过长，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。


# 数据结构

1. 快排的思想，最好最坏时间复杂度。

   基于分治法。取一个元素作为枢轴并备份，确定此时扫描的上界high和下界low，枢轴的位置就空出来了，然后在上下界的范围内交替从后到前找小于枢轴的元素和从前到后找大于枢轴的元素，放到上一个元素空出来的位置。最后2个指针相遇的位置就是枢轴最终的位置。然后对枢轴左右两边的子序列递归做同样的事情。O(nlog~2~n)\~O(n^2^)

   优化：选取数组开头，中间和结尾的元素，通过比较，选择中间的值作为快排的基准

3. B树、B+树

   B树又称多路平衡查找树，是针对外存设计的多路查找结构，M阶B树每个节点最多有M-1个关键字，并且以升序排列。子树的个数比关键字多一个，分布在每个关键字的两边，子树存放着大于左边关键字又小于右边关键字的关键字。如果找到叶节点就是查找失败了。

   B+树非叶节点的子树个数和关键字个数相等（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据）。所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶结点本身按大小排序组成一个链表）。无论查找是否成功，每次都是一条从根节点到叶节点的路径。

   据我所知mysql就是用B+树存储的

4. DFS、BFS

   DFS是深度优先搜索，就以树来举例，从根节点开始，向下搜索它的子树节点，再向下搜索子树的子树，一直到叶节点再回溯到上一个节点搜索没搜过的子树。通常用函数递归或者是栈的方法实现。

   BFS是优先搜索同一层的节点。通常用队列实现。过程就是队列出队一个节点，然后把该节点的所有未遍历的节点入队，由于队列先进先出的特点就导致看起来就像是一层一层地搜索节点

5. 擅长的编程语言，评价自己的编程能力。

   我比较擅长java，因为我大一大二的时候参加了很多工程项目，主要是负责后端这块的，也接触过常用的开发框架比如说spring boot这种。所以我的代码能力和代码规范也有了很大的提高。另外就是我大三为了准备蓝桥杯比赛，练习了很多算法题。所以我的对算法的理解能力也还可以。

6. 排序算法讲一讲有哪些

   插入排序有：

   1. 直接插入排序

      待排序元素左边是有序序列，右边是无序序列。需要把待排序元素存到其它变量，然后从这个位置向左边扫描边移动，找到待排序元素在有序表的相对位置并插入。O(n^2^)稳定

   2. 折半插入排序

      折半查找前面的有序表，找到位置后统一后移。O(n^2^) 稳定

   3. 希尔排序

      先将待排序表按照步长分为若干组，组内的元素在原表中相互间隔1个步长，对各个小组分别进行直接插入排序，然后缩小步长继续进行之前的步骤。这样可以减少大量的移动操作。O(n^1.3^)~O(n^2^)不稳定

   交换排序有：

   1. 冒泡排序

      从前往后两两比较相邻元素的值，若为逆序，则交换。保证指针始终指向的是已比较过的最大的那个，结果是将最大的元素交换到待排序列的最后一个位置。每趟冒泡的结果是把序列中的最大元素放到了序列的最终位置。从尾部产生有序序列。O(n^2^) 稳定

   2. 快速排序

      基于分治法。任取一个元素作为基准并备份，确定扫描的上界high和下界low，基准元素备份完后原来的位置就空出来了，然后在上下界的范围内交替从后到前找小于枢轴的元素和从前到后找大于枢轴的元素，放到上一个元素空出来的位置。最后2个指针相遇的位置就是枢轴最终的位置。然后对枢轴左右两边的子序列递归做同样的事情。O(nlog~2~n)不稳定

   选择排序有：

   1. 简单选择排序

      第i趟排序选择关键字最小的元素与第i个元素交换，确定一个元素的最终位置。第k轮找到关键字第k小的元素与第k个元素交换。与冒泡排序的区别是冒泡排序一轮排序可以改变多个元素的位置，而简单选择排序的移动次数少。O(n^2^) 不稳定

   2. 堆排序

      把一维数组看成是完全二叉树。大顶堆满足任意非根节点小于其双亲节点。过程就是不断将堆顶元素和未排序序列最右端元素互换，然后堆的范围-1并调整堆。好处是求前n个最大元素时的效率很高。O(nlog~2~n)不稳定

   归并排序：基于分治法，排序表视为n个有序的子表，每个子表的长度为1，然后两两归并，直到合并成一个长度为n的有序表为止。关键点是2个有序表合并的时间复杂度很低。稳定O(nlog~2~n)

   基数排序：比如以10为基数r，开10个队列，则首先从个位开始，如果个位为i就放到队列i中，全部分配后再进行收集，从个位小的队列依次收集，之后从第二位开始继续分配再收集，直到最高位。稳定.时间复杂度为最大位数*（数据量+基数）

   两个最小生成树算法包括？他们的具体实现过程是？都有什么优缺点？时间复杂度是？

   - Prim算法

     初始时从图中任取一顶点加入树T，之后选择一个与当前T中顶点距离最近的顶点，并将该顶点和相应的边加入T，每次操作后T中的顶点数和边数都增1。以此类推直至图中所有的顶点都并入T，得到的T就是最小生成树。此时T中必然有n-1条边。**适用于点少的图**,O(V^2^)

   - kruskal算法

     把每个顶点看成一个连通分量，然后按照边的权值由小到大的顺序开始选边，若该边依附的顶点落在T中不同的连通分量上，则将此边加入T。以此类推，直至T中所有顶点都在一个连通分量上。**适用于边少的图**,用堆优化可以到O(Elog~2~E)

   两个最短路径算法包括？详细描述实现过程。时间复杂度是？

   - Dijkstra算法单源最短路

     首先设个布尔类型的数组visit标记已求得的最短路径的顶点，源点标记一下，设数组D存放源点到其他点的最短路径长度，初始化为源点到该点相连的边的权值，若没有则设为无穷。然后从没标记过的点中选一点j满足D[j]最小，此时已经找到源点到j点的最短距离，把j标记，然后用j点松弛源点到没标记过的各点距离。

     不适用于负权边,O(n^2^)

   - Floyd算法求各顶点之间最短路

     有n个节点，不断用1到n的节点尝试作为中间节点缩短各顶点之间的距离,所有节点都尝试过一次后就得到了各顶点之间最短路

     O(n^3^)

   （霍夫曼树）哈夫曼编码描述一下实现过程。

   - 每个字符作为一个节点，出现的频率作为节点的权值，构造一个哈夫曼树，然后左子树对应的边写0，右子树对应的边写1，从根节点到叶节点的路径按顺序就构成了哈夫曼编码。没有一个编码是另一个编码的前缀。

   什么是时间复杂度？什么是空间复杂度？

   - 待处理数据量和处理时间之间的关系

# 计组

1. cache 的应用

   解决CPU和主存之间速度不匹配的问题。

2. 32位的计算机能存储的最大内存是多少？

   32位代表地址空间是2^32^,就是4G

## 中断

CPU在执行程序时，被内部或外部的事件所打断，转去执行一段预先安排好的中断服务程序。服务结束后又返回原来的断点，继续执行原来的程序。中断请求是外设随机向CPU提出的。

（1）系统中存在多种中断，CPU如何识别中断源？可以依靠向量中断（即中断源的识别标志）中断向量表以及中断查询。

（2）有多个中断同时请求，CPU如何应对？根据优先级进行判断。可以软件实现，也可以硬件实现。

（3）中断处理过程中，又有中断提出请求，怎么处理？根据优先权来进行判断。

# 主观题

为什么来我们学校

为什么选择读研而不是就业？

- 因为我想深入学习一些计算机前沿领域的知识，比如说边缘计算、物联网或者机器学习等等，这是我在本科阶段所欠缺的。华东师范的软件学院学科实力很强。

是否有科研经历

- 没有，因为我的主要精力放在了竞赛上，另外我个人认为我母校的资源不能很好的支持学生进行科研活动

# 概率论

1. 大数定理

   伯努利大数定律：当试验次数趋于无穷时,事件A发生的频率收敛于A发生的概率

2. 中心极限定理

   给定一个任意分布的总体。我每次从这些总体中随机抽取 n 个抽样，一共抽 m 次。 然后把这 m 组抽样分别求出平均值。 这些平均值的分布接近正态分布。

3. 全概率公式

   如果样本空间E有一个完备事件组A~1~~A~n~是它的一个划分，B是试验的一个事件，则B发生的概率=B和A1一起发生的概率+B和A2一起发生的概率...根据条件概率公式，又等于事件Ai发生概率乘以在事件Ai发生下B发生概率的累加，i从1到n。

4. 最大似然估计

   **利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值**

5. 贝叶斯的含义

   P(A|B)=P(B|A)*P(A)/P(B) 由先验概率求后验概率  A可以表示为原因事件，B表示为结果事件

# 数据库

三级模式：

（1）内模式（存储模式）：数据**物理结构**和**存储方式**的描述。

（2）模式（scheme，逻辑模式）：数据库中**全体数据**的**逻辑结构**和**特征描述**。

（3）外模式（用户模式）：**局部数据**的逻辑结构和特征描述。

二级映像：数据库实现三个抽象层次之间的联系和转换：

（1）外模式/模式映像：数据具有逻辑独立性，每一个外模式，都有一个外模式/模式映像。当模式发生改变时，只要改变其映射，就可以使外模式保持不变，对应的应用程序也可保持不变

（2）模式/内模式映像：数据具有物理独立性，定义了全局数据逻辑结构和存储结构之间的对应关系。当数据的存储结构发生变化时，只需改变概念模式一内模式映射，就能保持概念模式不变，因此应用程序也可以保持不变。

事务的ACID特性：

1. 原子性

   事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败

2. 一致性

   事务必须使数据库从一个一致性状态变换到另外一个一致性状态。所谓一致性状态就是正确的状态，满足约束的状态。原子性、隔离性、持续性都是为了达到这个状态而设计的，保证所有约束没有被打破。

3. 隔离性

   多个并发事务之间要相互隔离

4. 持续性

   事务一旦被提交，它对数据的改变就是永久性的

范式判定方法

- 1NF：所有属性不可再分。
- 2NF：在满足1NF的基础上，**消除了非主属性对于码的部分函数依赖**
- 3NF：在满足2NF的基础上，**消除了非主属性对于码的传递函数依赖**。
- BCNF：在满足3NF的基础上，消除**主属性**对于码的部分与传递函数依赖。

范式的作用：解决数据冗余，提高存储效率，保证数据有效性。可能会降低查询效率

# 线性代数

矩阵的**秩**

- 矩阵中列向量极大线性无关组包含向量的个数。

- 如果一部分向量乘以系数=另一些向量乘以系数而且系数不全为0就是向量的线性相关

- 如果用矩阵去解方程组就会发现矩阵存在一些冗余的向量，对于解方程组没有帮助

矩阵**可逆**等价于：

- 行（列）向量组线性无关

- 矩阵为满秩矩阵,秩=n

- 行列式不为0

# 操作系统

3. 死锁产生的原因

   - 因竞争资源发生死锁

   - 进程推进顺序不当发生死锁

   
   产生死锁的四个必要条件？
   
   （1）互斥条件：进程对所分配到的资源不允许其他进程进行访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源
   
   （2）请求和保持条件：进程获得一定的资源之后，又对其他资源发出请求，但是该资源可能被其他进程占有，此请求阻塞，但又对自己获得的资源保持不放
   
   （3）不可剥夺条件：是指进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用完后自己释放
   
   （4）环路等待条件：是指进程发生死锁后，必然存在一个进程–资源之间的环形链
   
   死锁避免：银行家算法
   
   - 当一个进程申请使用资源的时候，银行家算法通过先 **试探** 分配给该进程资源，然后通过安全性算法判断分配后的系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待。
   
   - 而安全性算法是指剩余资源够不够满足进程队列的某个进程，而在这个进程结束后加上该进程的资源能否满足进程队列剩下的某个进程，以此类推直到队列所有进程都可满足就是安全的
   
   进程与线程的区别
   
   - 进程和线程的根本区别是**进程**是操作系统**资源分配的基本单位**，而**线程**是**处理器调度的基本单位**。
   
   - 资源开销：每个进程都有独立的代码和数据空间，程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。
   
   - 包含关系：一个进程内有多个线程
   
   - 内存分配：同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的。
   
   进程通信的方式
   
   1. 管道通信
   
      所谓“管道"，就是一个缓冲区，一个进程往里写，一个进程往外读。管道是单向的数据通道，可以使用两个管道实现双向读写。如果管道数据没有被及时读取，则写进程会一直等待，通信效率低下。
   
   2. 消息传递
   
      进程把数据封装成格式化的消息放到接收进程的缓冲队列，或者是放到邮箱里就可以了，不必一直挂起等待消息被接收。而读数据的进程从消息队列里读就可以了。
   
   3. 共享存储
   
      然后因为消息队列拷贝数据耗时，以及存放和读取数据的速度差别可能大。
   
      所以提供一个共享存储，支持多个进程同时在这里存取，当然要使用信号量保证同步互斥关系。

# 物联网

我在无线传感网课程中读过Leach协议的源码，虽然细节很多都忘记了，但还记得leach协议是选簇头的协议，因为传感网节点能量有限，能量消耗和通信距离的三次方成正比，所以不能每个节点都直接和汇聚节点通信，这样节点能量消耗速度差别很大。因此通过聚簇的方式，簇内节点只和簇头通信，簇头再通过直接或跳转的方式将数据发给汇聚节点。Leach协议就是一种选择簇头的协议，它以循环的方式随机选择簇头节点，将整个网络的能量负载平均分配到每个传感器节点中，从而达到降低网络能源消耗、提高网络整体生存时间的目的

# 机器学习

决策树是什么？

- 决策树是一种**分类算法**，属于**监督学习**。决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。通过学习样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。

梯度下降算法

- 讲梯度下降算法之前首先讲一下损失函数，它是测量模型估计值与实际值之间误差的函数，变量是模型的参数，线性回归问题常用的损失函数有平方损失函数。我们训练模型的目的是找到尽可能让损失函数最低的参数，这就用到了梯度下降算法，它是一个不断迭代的过程，在每一次迭代中，对于每个参数，求损失函数在该参数方向上的偏导数再乘上一个学习率获得参数要修改的步长，让这个参数减去这个步长就可以了。最终能逼近一个最小值。当然因为线性回归函数是凸函数，能找到最小值，否则可能是极小值。

# 机试

第二道我觉得有点贪心算法的意思，我看出来关键是要尽可能消去A，所以我在第一个循环中不断找AP相消的，直到没有才在第二个循环消除PP

第三道是和子图有关，我想到用并查集来做但是时间好像不够了。