# 大纲：

- 最优分类超平面
- 高维映射和内积内核
- 支持向量机分类

# Introduction

学习问题的标准公式是对x的所有值估计一个函数(函数逼近)，y = f(x)。

# 最优分类超平面 Optimal separating hyperplane

- 我们将从最简单的例子开始:在可分离数据上训练的线性机器
  - 对一般情况的分析——在不可分离数据上训练的非线性机器——导致了一个非常类似的二次规划问题)。
- 最简单的分离平面是D(x)=0
- 该直线在平面上的点将满足所示的平面方程

通过a和垂直于n的平面的方程

<img src="D:\TyporaData\第四节、支持向量机\image-20200816144633973.png" alt="image-20200816144633973" style="zoom:50%;" />

## 最优的分离

存在许多可能的分离超平面。选择离任何训练示例最远的分离超平面。

超平面的位置通过与每个训练示例关联的权重来指定。

在超平面附近的例子接受非零权值，称为支持向量。

分离超平面

- 一个能够分离训练数据的线性函数(向量点积，w是法向量，w0与到原点的距离成比例)
- D(x)是到分离平面的距离
  - D(x) >= 0 for yi= 1 (1 class) and
  - D(x) < 0 for yi= -1(the other class)
- 设当y~i~= 1 (y~i~= -1)时，让τ~+~（τ~-~）为分离平面的最短距离
- 支持向量将是有D(x)=τ~+~(τ~-~)的向量。为了方便，我们可以按比例缩放w和w0来缩放缩放τ=1
- 注意，当线性可分的情况下，w, w0可以被缩放，所以下一个条件成立

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822150431380.png" alt="image-20200822150431380" style="zoom:67%;" align="left"/>

- 可以组合成		y~i~[(w.x~i~)+w~0~]>=1	i=1,…,n

- 总之

  D(x)=(w.x)+w~0~

  y~i~[(w.x~i~)+w~0~]>=1	i=1,…,n



- **margin**：τ 	 从分离的超平面到最近数据的最小距离
- 最优分类超平面
  
- 当margin为最大时
  
- 分离超平面与样本x '之间的距离为
  $$
  \frac{|D(x')|}{||w||}
  $$
  
- 所有的模式都服从这个不等式
  $$
  \frac{y_kD(x_k)|}{||w||}\geq\tau \qquad k=1,…,n
  $$

- 最大化**margin**=最小化||w||

$$
\tau=\frac{1}{||w||}
$$

- Support Vectors
  - 存在于边缘的数据(当满足相等条件时)
  - 在大多数情况下，支持向量的数量比模式的数量小得多

- 对于超平面上的这些点，它们组成了两个平面

  H~1~：(w.x~i~)+w~0~=1

  H~2~：(w.x~i~)+w~0~=-1

$$
间隔margin=\frac{2}{||w||}
$$

两个平面是平行的，它们之间没有点。

目标是使∥w∥最小化从而使2/∥w∥最大化。

二次优化问题
- 将目标函数最小化<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822160032129.png" alt="image-20200822160032129" style="zoom:50%;" />
- 服从<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822160102437.png" alt="image-20200822160102437" style="zoom:50%;" />

- 这是一个有线性约束的极小化二次函数
- 该解由d+ 1个参数组成

# 对偶问题

- 该解决方案由n个参数组成

- 如果成本函数和约束是凸的，则可转换

- 上述约束最小化问题(原问题)可用拉格朗日乘子求解。

  构建Lagrangian function

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822224010134.png" alt="image-20200822224010134" style="zoom:50%;" />

- 通过将函数的偏导数设为0，得到函数的鞍点，从而得到最优条件

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822224144053.png" alt="image-20200822224144053" style="zoom:50%;" />

- 求解最优超平面有:

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822224209604.png" alt="image-20200822224209604" style="zoom:50%;" />

​		对于w*，它是向量的线性组合

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822224235975.png" alt="image-20200822224235975" style="zoom:50%;" />

# 最优分类超平面

- 在鞍点处，对于每个拉格朗日乘子，迭代i*，该乘子与其约束的乘积消失(优化理论库恩-塔克条件)

- Kuhn-Tucker定理

  - 与非零的符合向量i*对应的数据为支持向量。

    <img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822224402114.png" alt="image-20200822224402114" style="zoom:50%;" />

  - 那些不在平面上的必须使它们的侧倾角为i* = 0，方程才能成立。

- 求q关于w的最小值;，并同时要求q的导数对所有的α~i~趋于0，服从约束α~i~>=0
- 这是一个凸二次规划问题，因为目标函数本身是凸的，而满足约束的点也构成一个凸集。
- 这意味着我们可以等效地解决以下“对偶”问题:

对偶问题

•最大化Q(α)，受制于Q相对于w和 w0的梯度趋于0的约束，也受制于α>= 0的约束。

# 对偶问题

为了构造，我们将最优参数代入拉格朗日方程:

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822225350460.png" alt="image-20200822225350460" style="zoom:50%;" />![image-20200822225401196](C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822225401196.png)

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822225350460.png" alt="image-20200822225350460" style="zoom:50%;" />![image-20200822225401196](C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822225401196.png)

上述(对偶方程)必须是最大的关于所有的α

# 总结

通过查找参数来最大化函数

<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822225452860.png" alt="image-20200822225452860" style="zoom:50%;" />

服从<img src="C:\Users\12548\Documents\GitHub\Notebooks\人工智能课程\第四节、支持向量机.assets\image-20200822225511597.png" alt="image-20200822225511597" style="zoom:50%;" />

这个不能解析求解，除非在所有点都是支持向量的特殊情况下。