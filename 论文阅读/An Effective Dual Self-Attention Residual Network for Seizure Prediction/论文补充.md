在之前的论文总结中我了解了整篇论文的大致流程，所以在补充中我想比较深入地学习其中的关键方法。

# 数据集

本次研究中定义发作前状态是癫痫发作前30分钟的时期，定义发作间状态为癫痫发作4小时前和发作结束4小时后的时期。

在癫痫发作前需要留出治疗干预的时间，这要求模型能够不依靠干预期的的数据就能够识别出发作前状态。本次研究设干预期为5分钟，干预期应当紧邻着发作期，此期间的数据从训练数据中删除。

# 数据预处理

由于癫痫是比较稀少的事件，它的间期记录数比前期记录数要多。而大多数机器学习算法假设不同种类的数据是均匀分布的，如果数据的数量不平衡，将获得偏向于较大数量类别的分类器。

为了解决这个问题，作者调整2种时期的采样步长，来获得同样数量的2种数据，具体方法如下：

定义发作前期信号的长度为M，发作间期信号的长度为N，设两类数据长度比K=M/N。设置采样窗口长度为S。发作前期数据按照S*K的步长采样，间期数据按照S的步长采样。发作前期和发作间期获得的采样片段个数分别为a和b，如下公式：

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211002141041949.png" alt="image-20211002141041949" style="zoom:50%;" />

不过我不是很理解这个公式，我觉得a=M/(S*K)，b=N/S。当然在K不是特别小的情况下和作者给出的公式差别不会很大。

由于K<1，发作前期的采样会有部分重叠，这也是作者提到的重叠采样(overlapping sampling)，与增大间期数据采样步长相比，缩小发作前期采样步长能获得更多训练数据。

采样完后，获得了数量大致相同的间期数据和前期数据。

然而对于信号来说，仅仅进行振幅/时间维度上的分析并不足以完整地描述其特征，可能会丢失信息。而使用频域分析可能获取不同频段上的强度。因此单一维度的时间或者频率都不能有效概括整个信号特征，需要进行时频分析。

作者在这里使用短时傅里叶变换来获得频域的数据。短时傅里叶变换是给信号在时域上加窗，把信号分成一小段一小段，分别做傅里叶变换。作者由此获得了时频域谱图，如下图。

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211002142658108.png" alt="image-20211002142658108" style="zoom: 67%;" />

图中可见，作者将5秒的采样数据分成0.5秒一小段进行短时傅里叶变换，横坐标是频率，纵坐标是时间，图中的颜色作者没有指明，我从网上查询的结果来看应该代表power的强弱。

图中也可以看出60Hz处有很明显的电线噪声，作者通过排除频率范围为57-63 Hz和117-123 Hz的成分去除，直流分量也被去除。不过我不太理解此处去除117-123 Hz与直流分量的作用。

# 模型

## 残差网络

通过查阅资料，我对于残差网络能够解决深层神经网络的退化问题的理解是这样的

残差网络在单纯的前向传播基础上增加了短路连接，如下图所示

<img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-e870eeaa15333fdda25e11e84b45a5df_1200x500.jpg&refer=http%3A%2F%2Fpic2.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1635769988&t=ca9a6582240549614e2e9df89cd57f65" alt="img" style="zoom: 25%;" />

假设H(x)是想要的映射，那么原来神经网络要用F(x)去拟合H(x)，现在增加了短路连接后，神经网络F(x)只需要去拟合H(x)-x，即恒等映射与实际值的残差，这部分神经网络所学习的效果最差不会差过恒等映射，只需要在x的基础上进行优化就可以了。而且短路连接没有要学习的参数，不会增加额外的除加法外的运算量，因此能够让多层的神经网络至少不会比层数少的差。因此作者在本次研究中使用残差网络。

## 自注意力机制

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211002222400224.png" alt="image-20211002222400224" style="zoom:50%;" />

频谱注意模块输入的数据是**ResNet得到的特征图X**，大小为C\*H\*W。C是通道，H 和W是频谱图的时间和频率。要把它把它输入卷积层产生2个新的特征映射**Y和Z**，大小同样为C\*H\*W。然后将Y和Z **reshape**到C\*N，N=H*W。然后将Y**转置**成N\*C，与Z进行矩阵乘法，得到**权重矩阵S**，形状为N\*N。该过程可以参考下图：

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211002223055686.png" alt="image-20211002223055686" style="zoom: 67%;" />

由于reshape操作相当于将特征图Y或Z从三维展平到二维，每个(c,n)对应着原来的(c,h,w)，在进行Y^T^*Z后，得到的**权重矩阵S**的每个元素S~ij~代表输入特征图第j个元素对第i个元素的影响。这个操作使得注意力机制能获得特征图中任意两个位置的**全局依赖关系**。

下一步还要把权重矩阵S进行softmax归一化，根据作者给出的公式<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003104649383.png" alt="image-20211003104649383" style="zoom:50%;" />，是对列的归一化。归一化后**每列的之和为1**，对于(i,j)位置即可理解为第i位置对j位置的权重或者说相关性，所有的i对j位置的权重之和为1，两个位置的特征表示越相似，它们之间的相关性就越大。

下一步一个让X经过卷积层的**新的特征图T**，形状为C\*H\*W，经过 reshape到C\*N，这个矩阵T再和形状为N\*N的权重矩阵S相乘。如下图

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003110048199.png" alt="image-20211003110048199" style="zoom:50%;" />

**T**的每一行与**权重矩阵S**中的每一列点乘，相当于将权重施加于T上。乘后得到C\*N的矩阵再reshape到C\*H\*W。这是这个模块所学习到的**attention**。最后一步还要将这个attention矩阵乘上学习率α并加上原始的特征图**X**（**这一步是传统注意力机制没有的，我觉得可能是作者收到残差思想的启发而设计的**）如该公式<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003110514557.png" alt="image-20211003110514557" style="zoom:50%;" />，得到最终结果E∈R^C*H*W^。α是经过学习得到的，初始α为0，输出即原始特征图X，随着学习的深入，在原始特征图上增加了加权的attention，得到特征图中任意两个位置的全局依赖关系。E是频谱加权特征与原始特征的和，具有全面的背景视角，并可根据频谱注意图选择性地收集全局信息。

通道注意模块与频谱注意模块相似，因此不再分析。

卷积神经网络的卷积核的感受野是局部的，要经过累积很多层之后才能把整个图像不同部分的区域关联起来。而Self-Attention机制具有全面的背景视角，这应该是作者添加Self-Attention模块的理由。

作者最终将这两个注意模块与原始特征全都进行element-wise求和，通过平均池化得到最终的特征图。

# 留一交叉验证

如果一个病人有n次癫痫以及t小时的发作间期的记录数据，整个发作间期记录被分成n个部分，每个部分有大约t/n小时，并随机与一个发作前记录分组。这一过程做n次，每一次一对前期-间期被保留进行测试，剩下的n-1对用来训练。

# 分类模型评价指标

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003115400313.png" alt="image-20211003115400313" style="zoom: 67%;" />

本文作为癫痫前期与间期的二分类模型研究，如果把癫痫前期设为阴性，癫痫间期设为阳性，则Sensitivity表示癫痫间期被正确识别的概率，specificity表示癫痫前期被正确识别的概率。

## Sensitivity

Sensitivity意义是哪些真实的阳性被正确识别。<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003115610520.png" alt="image-20211003115610520" style="zoom: 33%;" />

## specificity

specificity意义是哪些真实的阴性被正确识别。<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003115817735.png" alt="image-20211003115817735" style="zoom:33%;" />

## accuracy

accuracy意义是正确识别的样本数占总样本数的比值，不考虑预测的样本是正例还是负例<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003115912636.png" alt="image-20211003115912636" style="zoom:33%;" />

## AUC

AUC即ROC曲线下的面积。

横轴FPR: FP/(FP+TN)，FPR越大，预测正类中实际负类越多。

纵轴TPR：TP/(TP+FN)，Sensitivity(正类覆盖率),TPR越大，预测正类中实际正类越多。

曲线上的每个点代表着某个阈值下模型在FPR与TPR上的表现。

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20211003120819910.png" alt="image-20211003120819910" style="zoom:33%;" />

理想目标：TPR=1，FPR=0,即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好。

从AUC判断分类器（预测模型）优劣的标准：

- AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。
- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
- AUC = 0.5，跟随机猜测一样，模型没有预测价值。
- AUC < 0.5，比随机猜测还差。

因此AUC越高，ROC曲线离中线越远，模型就越好。