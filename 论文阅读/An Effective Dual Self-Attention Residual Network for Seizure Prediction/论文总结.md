选择论文：An Effective Dual Self-Attention Residual Network for Seizure Prediction

# 一、论文主题 idea

基于双重注意力残差网络(RDANet)，使用脑电信号数据训练一个癫痫发作预测模型。

双重：频谱注意模块+通道注意模块。

目标：区分癫痫发作间期和发作前期。

# 二、实验方法

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20210930105320025.png" alt="网络模型" style="zoom:50%;" />

数据来源：CHB-MIT 头皮脑电波数据集，22名儿童受试者数据，连续记录844小时，以256 Hz的采样频率从22个电极上采集的脑电波信号。

实验步骤：

1. 使用短时傅立叶变换，将原始脑电波信号转换成代表时频特征的频谱图(time/frequency)。
2. 数据预处理：调整采样的滑动步长解决数据不平衡的问题；去除60hz的电线噪声。
3. 建立模型：残差网络(4个残差块)+频谱注意模块+通道注意模块。
4. 训练：使用留一交叉验证(leave- one-out cross-validation)来评价预测结果，通过早停止(early-stop)解决过拟合。

残差块设计如下图：

<img src="https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20210930112631291.png" alt="image-20210930112631291" style="zoom:50%;" />

每个残差块由2个3*3卷积层，批归一化层，ReLu激活层组成。另外还添加有2个路径，即残差路径F(x)和单位映射x。

# 三、结论

本文使用四个参数来评价所提出的模型的性能:**灵敏度、特异性、准确性和AUC**。AUC是一种常用的评价分类任务性能的指标，它是接收器操作特性曲线(ROC)下的面积,面积越大，诊断准确性越高。

![image-20210930111631304](https://raw.githubusercontent.com/SNIKCHS/MDImage/main/img/image-20210930111631304.png)

总体而言，RDANet模型的性能评价高于CNN和ResNet模型。ResNet模型的预测结果比CNN模型更优，是因为ResNet模型通过构建**更深层次的网络和快速连接**，具有更强的表达能力。

本文的RDANet模型与ResNet模型对比，可以看出引入双注意力模块后，灵敏度比ResNet模型提高了0.17%，特异性降低了0.09%(原文为提高，怀疑作者笔误)，AUC值提高了0.68%，准确性提高了0.37%。

# 四、概念学习

## 1. 注意力机制

注意力机制可以帮助模型对输入的X每个部分赋予不同的权重，抽取出更加关键及重要的信息，使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销。

我了解到卷积神经网络里注意力机制主要分为两种，一种是spatial attention(空间注意), 另外一种是channel attention(通道注意)。在该论文中由于处理的不是图片而是频谱图（时间/频率），作者改成了Spectrum attention(频谱注意)+Channel attention(通道注意)。

本文中注意力模块要处理的数据是C * H * W的特征图，实际实验的输入为22\*9\*114，C是通道，代表卷积核的数量，H 和W就是频谱图的维度(时间/频率)。频谱注意就是对于所有的通道，在二维平面上，对H * W尺寸的频谱图学习到一个权重，对每个频谱单位(H * W)都会学习到一个权重。在通道维度上，权重都是一样的，但是在平面上权重不一样。

与频谱注意相反，通道注意就是对每个C，在通道维度上，学习到不同的权重，平面维度上权重相同。

可以理解为频谱注意模块融合了脑电波信号的局部特征和全局特征，通道注意模块挖掘信道映射之间相互依赖关系。

本文最后还将这两个注意模块进行对位元素相加(element-wise sum)。与原始特征融合后，通过平均池化得到最终的特征图。

## 2. 卷积神经网络

1. 卷积层

   卷积层的目标是提取输入数据的特征。这是通过卷积核在输入图像上滑动进行卷积运算得到的。具有不同卷积核的卷积可以执行如边缘检测，模糊和锐化的操作。而卷积核的参数也会不断的通过反向传播更新。

2. 激活层

   我认为机器学习的目的是学习一个合适的拟合函数，拟合现实中某些现象、规律的真实函数，从而进行有效预测或分类。而神经网络随着层数的增加，理论上可以逼近任何函数。如同数学上的泰勒展开用函数在某一点的各阶导数值做系数构建一个多项式来近似表达这个函数，神经网络的激活层承担这个任务。激活层通常使用非线性函数如ReLU，因为线性函数的多层累加效果相当于一层线性函数，而现实问题总有线性函数不能拟合的情况。

3. 池化层

   池化层有Max Pooling 最大池化和Average Pooling平均池化，我的理解是该层能减少数据量，忽略不重要的特征也避免了过度拟合

## 3. 残差网络

卷积神经网络随着层数增多，能够提取到的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。

但如果单纯的增加网络的深度，会导致梯度消失或梯度爆炸。同时也会有退化问题，其原因是解空间过于复杂，随机梯度下降算法无法达到最优解。

而ResNet最重要的残差模块从输入直接引入一个短连接到非线形层的输出上，变为H(x)=F(x)+x。

我从网络上看到2个结论：

1. 带短连接的网络的拟合高维函数的能力比普通连接的网络更强。
2. 残差能够缓解深层网络的训练问题，有非常好的反向传播特性。

目前还不是很能够理解，计划在以后继续学习。